# -*- coding: utf-8 -*-
"""CNN-Skyrmiones.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SH7NePl2PLtC56C2BL7E9PSiHNDFRHNE
"""

import os
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.utils.data import DataLoader, random_split
from torchvision import datasets, transforms
from datetime import datetime
from tqdm import tqdm
from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt
import json


# =====================
# 1. Configuración
# =====================
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Entrenando en:", device)

batch_size = 32
epochs = 15
img_size = 224  # tamaño de entrada para la CNN

# =====================
# 2. Transformaciones
# =====================
rgb_transform = transforms.Compose([
    transforms.Resize((img_size, img_size)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]) # normalización
])

# =====================
# 3. Dataset y DataLoader
# =====================
dataset_data_dir = "/home/mitchellmirano/Desktop/MitchellProjects/H vs J phase diagram/DATASET"

full_dataset = datasets.ImageFolder(root=dataset_data_dir, transform=rgb_transform)

# dividir en train/test (80/20)
train_size = int(0.8 * len(full_dataset))
test_size  = len(full_dataset) - train_size
train_dataset, test_dataset = random_split(full_dataset, [train_size, test_size])

model2_train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
model2_test_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

print("Clases detectadas:", full_dataset.classes)

label2id = {c: i for i, c in enumerate(full_dataset.classes)}
id2label = {i: c for i, c in enumerate(full_dataset.classes)}

with open('classes/id2label_cnn.json', 'w') as f:
    json.dump(id2label, f)

with open('classes/label2id_cnn.json', 'w') as f:
    json.dump(label2id, f)

# =====================
# 4. Definir CNN
# =====================
class SimpleCNN(nn.Module):
    def __init__(self):
        super(SimpleCNN, self).__init__()
        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)
        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)
        self.pool = nn.MaxPool2d(2, 2)
        self.fc1 = nn.Linear(64 * (img_size//4) * (img_size//4), 128)
        self.fc2 = nn.Linear(128, 4)

    def forward(self, x):
        x = self.pool(F.relu(self.conv1(x)))   # salida -> [B, 32, H/2, W/2]
        x = self.pool(F.relu(self.conv2(x)))   # salida -> [B, 64, H/4, W/4]
        x = torch.flatten(x, 1)
        x = F.relu(self.fc1(x))
        x = self.fc2(x)
        return x

num_classes = len(full_dataset.classes)
model = SimpleCNN().to(device)

# =====================
# 5. Pérdida y Optimizador
# =====================
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# =====================
# 6. Entrenamiento
# =====================
start_train = datetime.now()

for epoch in range(epochs):
    # --- Modo entrenamiento ---
    model.train()
    running_loss = 0.0

    pbar = tqdm(model2_train_loader, desc=f"Epoch {epoch+1}/{epochs} [Entrenando]")
    for images, labels in pbar:
        images, labels = images.to(device), labels.to(device)

        optimizer.zero_grad()
        outputs = model(images)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        running_loss += loss.item()

    avg_loss = running_loss / len(model2_train_loader)

    # --- Modo evaluación ---
    model.eval()
    correct, total = 0, 0
    with torch.no_grad():
        for images, labels in model2_test_loader:
            images, labels = images.to(device), labels.to(device)
            outputs = model(images)
            _, predicted = torch.max(outputs, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

    test_accuracy = 100 * correct / total

    print(f"Epoch {epoch+1}/{epochs} | Loss: {avg_loss:.4f} | Test Accuracy: {test_accuracy:.2f}%")

    if test_accuracy > 95:
        break

end_train = datetime.now()
delta = end_train - start_train

print(f"\n⏱️ Tiempo total de entrenamiento: {delta.total_seconds()/60:.2f} minutos")

# =====================
# 7. Evaluación
# =====================
model.eval()
correct, total = 0, 0
all_preds, all_labels = [], []

start = datetime.now()
with torch.no_grad():
    for images, labels in tqdm(model2_test_loader, desc="Evaluación"):
        images, labels = images.to(device), labels.to(device)
        outputs = model(images)
        _, predicted = torch.max(outputs, 1)

        total += labels.size(0)
        correct += (predicted == labels).sum().item()

        all_preds.extend(predicted.cpu().numpy())
        all_labels.extend(labels.cpu().numpy())

end = datetime.now()
delta = end - start

print(f"Accuracy en test: {100 * correct / total:.2f}%")
print(f"Inference time: {delta.total_seconds()/60:.3f} minutes")


class_report = classification_report(all_labels, all_preds, target_names=train_dataset.dataset.classes,digits=3)

with open('results/SkyrmionClassifierCNN.txt', 'w') as f:
    f.write(class_report)
print(class_report)

conf_matrix = confusion_matrix(all_labels, all_preds)
sns.heatmap(conf_matrix, annot=True, fmt='d',
            cmap='Blues',
            xticklabels=train_dataset.dataset.classes,
            yticklabels=train_dataset.dataset.classes)
plt.xlabel('Actual')
plt.ylabel('Predicted')
plt.savefig('results/SkyrmionClassifierCNN.png')



torch.save(model,'models/SkyrmionClassifierCNN')



# torch.save(model,'/content/drive/MyDrive/CancerMama/Models/Assembled/VIT-Google')

# model = torch.load('/content/drive/MyDrive/CancerMama/Models/Assembled/VIT-Google')